{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8867744,"sourceType":"datasetVersion","datasetId":5336829}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Aim is to predict the marks of students of the test data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-07T04:47:50.022735Z","iopub.execute_input":"2024-07-07T04:47:50.023194Z","iopub.status.idle":"2024-07-07T04:47:51.269829Z","shell.execute_reply.started":"2024-07-07T04:47:50.023156Z","shell.execute_reply":"2024-07-07T04:47:51.268629Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Use the file namd 'training data' to train the model\n\ndata = pd.read_excel('/kaggle/input/dataset/Training data.xlsx')\nx_train = np.array(data.iloc[:,0:8])\ny_train = np.array(data.iloc[:,8]).reshape(-1,1)\n\n# Try plotting y_train with different features\n# To get an idea whether to add some features or not\n# Add some features if required in x_train\n\n# Also do label encoding for features not represented in numbers\n# refer the link if not know : https://youtu.be/589nCGeWG1w?si=t2Wa7LgbUOO4RooM\n\ndef feature_changing(x_train):\n  # ---------\n    x_train[x_train=='yes']=1\n    x_train[x_train=='no']=0\n    x_train[x_train=='M']=1\n    x_train[x_train=='F']=0\n  # ---------\n    return x_train\n\nx_train = feature_changing(x_train)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:47:51.272174Z","iopub.execute_input":"2024-07-07T04:47:51.272784Z","iopub.status.idle":"2024-07-07T04:47:51.857403Z","shell.execute_reply.started":"2024-07-07T04:47:51.272742Z","shell.execute_reply":"2024-07-07T04:47:51.856078Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def z_score(x_train):\n\n  # ---------\n    # write the code for feature scaling here\n    x_mean = np.mean(x_train, axis=0)\n    x_std = np.std(x_train, axis=0)\n    \n    x_train=(x_train - x_mean)/x_std\n  # ---------\n\n    return x_train,x_std,x_mean\nx_train = x_train.astype(np.float64)","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:47:51.858760Z","iopub.execute_input":"2024-07-07T04:47:51.859249Z","iopub.status.idle":"2024-07-07T04:47:51.867074Z","shell.execute_reply.started":"2024-07-07T04:47:51.859215Z","shell.execute_reply":"2024-07-07T04:47:51.865554Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def cost(x_train,y_train,w,b):\n\n  # ---------\n    y_pred=np.dot(x_train,w)+b\n    # Use mean square error as cost function\n    cost=0.5*np.mean((y_pred-y_train)**2)\n  # ---------\n\n    return cost","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:47:51.870095Z","iopub.execute_input":"2024-07-07T04:47:51.870569Z","iopub.status.idle":"2024-07-07T04:47:51.880031Z","shell.execute_reply.started":"2024-07-07T04:47:51.870524Z","shell.execute_reply":"2024-07-07T04:47:51.878787Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(x_train,y_train,w,b):\n    a=0.001\n  # ---------\n    m=len(y_train)\n    y_pred=np.dot(x_train,w)+b\n    dW = (1/m) * np.dot(x_train.T, (y_pred - y_train))\n    dB = (1/m) * np.sum(y_pred - y_train)\n    # Choose learning rate yourself\n    w=w-a*dW\n    b=b-a*dB\n  # ---------\n\n    return w,b","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:47:51.881672Z","iopub.execute_input":"2024-07-07T04:47:51.882226Z","iopub.status.idle":"2024-07-07T04:47:51.893394Z","shell.execute_reply.started":"2024-07-07T04:47:51.882184Z","shell.execute_reply":"2024-07-07T04:47:51.892202Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.astype(np.float64)\nx_train,x_std,x_mean = z_score(x_train)\nnp.random.seed(2147483647)\nw = np.random.randn(x_train.shape[1],1)\nb = np.random.randn(1)\n\nold_cost = 0\nwhile True:\n    current_cost = cost(x_train, y_train, w, b)\n    if abs(old_cost - current_cost) < 0.00001:\n        break\n    old_cost = current_cost\n    w, b = gradient_descent(x_train, y_train, w, b)\nprint(w)      \nx_predict = pd.read_excel('/kaggle/input/dataset/Test data.xlsx').iloc[:,:8].to_numpy()\nx_predict = feature_changing(x_predict)\nx_predict = (x_predict - x_mean)/x_std\nans = pd.read_excel('/kaggle/input/dataset/Test data.xlsx').iloc[:,8].to_numpy()\n\ny_predict = np.dot(x_predict,w) + b\n\naccuracy = 0\nfor dim in range(len(ans)):\n  if abs(y_predict[dim]-ans[dim])<0.5: # do not change the tolerance as you'll be checked on +- 0.5 error only\n    accuracy += 1\naccuracy = round(accuracy*100/200.0,2)\nok = 'Congratulations' if accuracy>95 else 'Optimization required'\nprint(f\"{ok}, your accuracy is {accuracy}%\")","metadata":{"execution":{"iopub.status.busy":"2024-07-07T04:47:51.895457Z","iopub.execute_input":"2024-07-07T04:47:51.895891Z","iopub.status.idle":"2024-07-07T04:47:52.579603Z","shell.execute_reply.started":"2024-07-07T04:47:51.895850Z","shell.execute_reply":"2024-07-07T04:47:52.578302Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[[-1.39289013e+00]\n [-3.97122964e-03]\n [-8.97101797e-01]\n [ 3.45756555e+00]\n [-1.26779654e+00]\n [-1.47437588e+00]\n [ 1.61491949e-01]\n [ 5.34494975e+00]]\nCongratulations, your accuracy is 100.0%\n","output_type":"stream"}]}]}